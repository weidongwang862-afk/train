# scripts/09S_train_residual_corrector_ctx.py
from __future__ import annotations

import os
import json
from pathlib import Path
from typing import Dict, Tuple, List

import numpy as np
import pandas as pd

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.optim.lr_scheduler import CosineAnnealingLR


# =========================
# Paths (match your project)
# =========================
OUT_ROOT = Path(r"E:\RAW_DATA\outputs")
SEQ_DIR = OUT_ROOT / "09_seq_featcore"
VIN_DIR = SEQ_DIR / "vin_npz"
RES_DIR = SEQ_DIR / "09_residual"

STATS_JSON = SEQ_DIR / "stats.json"

# residual index files (generated by Step B)
# residual_index_train_OOF_pred_gated_any.csv etc.
BASE_COL = os.environ.get("BASE_COL", "pred_core").strip()  # pred_core | pred_gated_any

TR_CSV = RES_DIR / f"residual_index_train_OOF_{BASE_COL}.csv"
VA_CSV = RES_DIR / f"residual_index_val_OOF_{BASE_COL}.csv"
TE_CSV = RES_DIR / f"residual_index_test_OOF_{BASE_COL}.csv"

# =========================
# Runtime / Hyperparams
# =========================
DEVICE = os.environ.get("DEVICE", "cuda" if torch.cuda.is_available() else "cpu")
SEED = int(os.environ.get("SEED", "42"))

MODEL = os.environ.get("MODEL", "tfmr").strip().lower()  # gru | mlp | tcn | tfmr

BATCH_SIZE = int(os.environ.get("BATCH_SIZE", "512"))
EPOCHS = int(os.environ.get("EPOCHS", "15"))
LR = float(os.environ.get("LR", "1e-3"))
WEIGHT_DECAY = float(os.environ.get("WEIGHT_DECAY", "1e-4"))

# model sizes
HIDDEN = int(os.environ.get("HIDDEN", "96"))          # for GRU/TCN/TFMR embedding
LAYERS = int(os.environ.get("LAYERS", "2"))           # GRU layers / TFMR layers
DROPOUT = float(os.environ.get("DROPOUT", "0.10"))

# Transformer params
D_MODEL = int(os.environ.get("D_MODEL", "128"))
NHEAD = int(os.environ.get("NHEAD", "4"))
FFN = int(os.environ.get("FFN", "256"))

# gamma search on val
GAMMA_MAX = float(os.environ.get("GAMMA_MAX", "1.0"))
GAMMA_STEPS = int(os.environ.get("GAMMA_STEPS", "41"))  # 0..2 step 0.05

# Optional: emphasize low-SoH residuals (uses y_true)
LOW_THR = float(os.environ.get("LOW_THR", "0.90"))
LOW_W = float(os.environ.get("LOW_W", "1.0"))  # 1.0 disables

OUT_TAG = os.environ.get("OUT_TAG", f"RESCTX_OOF_{BASE_COL}_{MODEL}").strip()


def set_seed(seed: int):
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


def _need_file(fp: Path, name: str):
    if not fp.exists():
        raise FileNotFoundError(f"[missing] {name}: {fp}")


def _need_cols(df: pd.DataFrame, cols: List[str], name: str):
    miss = [c for c in cols if c not in df.columns]
    if miss:
        raise RuntimeError(f"[{name}] missing columns: {miss} | got={list(df.columns)}")


def mae_np(y: np.ndarray, p: np.ndarray) -> float:
    return float(np.mean(np.abs(y - p)))


# =========================
# Dataset
# =========================
class ResidualSeqDataset(Dataset):
    """
    Reads residual_index_*.csv with columns:
      vin, t_idx, t_end, y_true, base_pred, residual
    Loads vin_npz/{vin}.npz with arrays:
      X: (T, d_in) float32
    Returns:
      x_win: (L, d_in), base_pred: (1,), y_true: scalar, resid: scalar, vin, t_end, t_idx
    """
    # [新增] 接收 base_mu, base_std
    def __init__(self, df: pd.DataFrame, seq_len: int, base_mu: float, base_std: float):
        self.df = df.reset_index(drop=True)
        self.seq_len = int(seq_len)
        self.base_mu = base_mu
        self.base_std = base_std
        self.cache: Dict[str, np.lib.npyio.NpzFile] = {}

    def _load_vin(self, vin: str):
        if vin in self.cache:
            return self.cache[vin]
        fp = VIN_DIR / f"{vin}.npz"
        if not fp.exists():
            raise FileNotFoundError(f"Missing vin npz: {fp}")
        obj = np.load(fp)
        self.cache[vin] = obj
        return obj

    def __len__(self):
        return len(self.df)

    def __getitem__(self, i: int):
        vin = str(self.df.at[i, "vin"])
        t_idx = int(self.df.at[i, "t_idx"])
        t_end = int(self.df.at[i, "t_end"])

        y_true = float(self.df.at[i, "y_true"])
        base_pred = float(self.df.at[i, "base_pred"])
        resid = float(self.df.at[i, "residual"])
        # [新增] 这里的标准化至关重要！
        base_pred_norm = (base_pred - self.base_mu) / self.base_std

        obj = self._load_vin(vin)
        X = obj["X"]  # (T, d)

        L = self.seq_len
        s = t_idx - L + 1
        e = t_idx + 1
        # Safety: if index invalid, fail loudly with context
        if s < 0 or e > X.shape[0]:
            raise IndexError(f"Bad window: vin={vin} t_idx={t_idx} seq_len={L} X_len={X.shape[0]} s={s} e={e}")

        x_win = X[s:e].astype(np.float32, copy=False)

        return (
            torch.from_numpy(x_win),
            torch.tensor([base_pred_norm], dtype=torch.float32),  # 传标准化后的值
            torch.tensor(y_true, dtype=torch.float32),
            torch.tensor(resid, dtype=torch.float32),
            vin,
            t_end,
            t_idx,
        )


# =========================
# Models (inject base_pred as context at head)
# =========================
class GRUEncoder(nn.Module):
    def __init__(self, d_in: int, hidden: int, layers: int, dropout: float):
        super().__init__()
        self.gru = nn.GRU(
            input_size=d_in,
            hidden_size=hidden,
            num_layers=layers,
            batch_first=True,
            dropout=dropout if layers > 1 else 0.0,
            bidirectional=False,
        )
        self.ln = nn.LayerNorm(hidden)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        out, _ = self.gru(x)          # (B,L,H)
        h = out[:, -1, :]             # (B,H)
        h = self.ln(h)
        return h


class MLPEncoder(nn.Module):
    def __init__(self, d_in: int, hidden: int, dropout: float):
        super().__init__()
        # pool over time -> (B,d_in)
        self.net = nn.Sequential(
            nn.Linear(d_in, hidden),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden, hidden),
            nn.GELU(),
        )
        self.ln = nn.LayerNorm(hidden)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (B,L,d) -> mean pool
        z = x.mean(dim=1)
        h = self.net(z)
        return self.ln(h)


class TCNBlock(nn.Module):
    def __init__(self, c: int, k: int, dropout: float, dilation: int):
        super().__init__()
        pad = (k - 1) * dilation
        self.conv1 = nn.Conv1d(c, c, kernel_size=k, dilation=dilation, padding=pad)
        self.conv2 = nn.Conv1d(c, c, kernel_size=k, dilation=dilation, padding=pad)
        self.dropout = nn.Dropout(dropout)
        self.ln = nn.LayerNorm(c)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (B,C,L)
        y = self.conv1(x)
        y = F.gelu(y)
        y = self.dropout(y)
        y = self.conv2(y)
        y = F.gelu(y)
        y = self.dropout(y)

        # remove left padding effect to keep causal-like alignment
        # (because padding added both sides in Conv1d, we crop to original length)
        if y.shape[-1] > x.shape[-1]:
            y = y[..., -x.shape[-1]:]

        z = x + y
        # LayerNorm expects (B,L,C)
        z2 = z.transpose(1, 2)
        z2 = self.ln(z2).transpose(1, 2)
        return z2


class TCNEncoder(nn.Module):
    def __init__(self, d_in: int, hidden: int, layers: int, dropout: float):
        super().__init__()
        self.proj = nn.Conv1d(d_in, hidden, kernel_size=1)
        blocks = []
        for i in range(layers):
            blocks.append(TCNBlock(hidden, k=3, dropout=dropout, dilation=2**i))
        self.blocks = nn.ModuleList(blocks)
        self.ln = nn.LayerNorm(hidden)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (B,L,d) -> (B,d,L) -> proj -> blocks
        z = x.transpose(1, 2)
        z = self.proj(z)
        for b in self.blocks:
            z = b(z)
        # take last time step (B,C,L)->(B,C)
        h = z[..., -1]
        return self.ln(h)


def causal_mask(L: int, device: torch.device):
    return torch.triu(torch.ones(L, L, device=device), diagonal=1).bool()


class TFMRDecoder(nn.Module):
    def __init__(self, d_in: int, d_model: int, nhead: int, nlayers: int, ffn: int, dropout: float, max_len: int = 4096):
        super().__init__()
        self.proj = nn.Linear(d_in, d_model)
        self.pos = nn.Parameter(torch.zeros(1, max_len, d_model))
        enc_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=ffn,
            dropout=dropout,
            batch_first=True,
            activation="relu",
            norm_first=True,
        )
        self.enc = nn.TransformerEncoder(enc_layer, num_layers=nlayers)
        self.ln = nn.LayerNorm(d_model)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (B,L,d)
        B, L, _ = x.shape
        h = self.proj(x) + self.pos[:, :L, :]
        m = causal_mask(L, h.device)
        z = self.enc(h, mask=m)     # (B,L,d_model)
        last = z[:, -1, :]
        return self.ln(last)


class ResidualCorrector(nn.Module):
    """
    Generic: encoder(x_seq)->h, then concat base_pred as context -> head -> residual_hat (standardized)
    """
    def __init__(self, encoder: nn.Module, h_dim: int, dropout: float):
        super().__init__()
        self.encoder = encoder
        self.head = nn.Sequential(
            nn.Linear(h_dim + 1, h_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(h_dim, 1),
        )

    def forward(self, x_seq: torch.Tensor, base_pred: torch.Tensor) -> torch.Tensor:
        # base_pred: (B,1)
        h = self.encoder(x_seq)  # (B,H)
        z = torch.cat([h, base_pred], dim=1)  # (B,H+1)
        out = self.head(z).squeeze(-1)        # (B,)
        return out


# =========================
# Eval helpers
# =========================
@torch.no_grad()
def predict_all(
    model: nn.Module,
    loader: DataLoader,
    device: str,
    resid_mu: float,
    resid_std: float,
    base_mu: float,    # [新增] 需要 base 的统计量来还原
    base_std: float,   # [新增]
) -> pd.DataFrame:
    model.eval()
    rows = []
    for x, base, y_true, resid_true, vin, t_end, t_idx in loader:
        x = x.to(device)
        base = base.to(device)

        pred_std = model(x, base)  # 输出是标准化的残差
        pred_std = pred_std.detach().cpu().numpy().astype(np.float32)

        # [关键修复] base 现在是标准化的 (Z-score)，必须还原回原始值！
        base_norm_np = base.detach().cpu().numpy().reshape(-1).astype(np.float32)
        base_raw_np = base_norm_np * base_std + base_mu 
        
        y_np = y_true.detach().cpu().numpy().astype(np.float32)
        r_true_np = resid_true.detach().cpu().numpy().astype(np.float32)

        r_pred_np = pred_std * resid_std + resid_mu  # 残差反标准化

        # [修复警告] Tensor 转 Numpy 的正确姿势
        t_idx_np = t_idx.detach().cpu().numpy().astype(np.int64)
        t_end_np = t_end.detach().cpu().numpy().astype(np.int64)

        rows.append(pd.DataFrame({
            "vin": list(vin),
            "t_idx": t_idx_np,
            "t_end": t_end_np,
            "y_true": y_np,
            "base_pred": base_raw_np,  # 这里存还原后的原始值
            "residual_true": r_true_np,
            "residual_pred": r_pred_np,
        }))
    return pd.concat(rows, ignore_index=True)


def pick_best_gamma(df: pd.DataFrame, gamma_max: float, steps: int) -> Tuple[float, float]:
    # choose gamma on VAL to minimize MAE of final_pred
    y = df["y_true"].to_numpy(dtype=np.float32)
    b = df["base_pred"].to_numpy(dtype=np.float32)
    rhat = df["residual_pred"].to_numpy(dtype=np.float32)

    gammas = np.linspace(0.0, gamma_max, steps, dtype=np.float32)
    best_g, best_mae = 0.0, 1e9
    for g in gammas:
        p = b + g * rhat
        m = mae_np(y, p)
        if m < best_mae:
            best_mae = m
            best_g = float(g)
    return best_g, float(best_mae)


def compute_metrics(df: pd.DataFrame, gamma: float) -> Dict[str, float]:
    y = df["y_true"].to_numpy(dtype=np.float32)
    b = df["base_pred"].to_numpy(dtype=np.float32)
    r_true = df["residual_true"].to_numpy(dtype=np.float32)
    r_hat = df["residual_pred"].to_numpy(dtype=np.float32)

    base_mae = mae_np(y, b)
    final_mae = mae_np(y, b + gamma * r_hat)
    resid_mae = mae_np(r_true, gamma * r_hat)
    return {"base_MAE": base_mae, "final_MAE": final_mae, "resid_MAE": resid_mae}


# =========================
# Training
# =========================
def main():
    set_seed(SEED)

    _need_file(STATS_JSON, "stats.json")
    _need_file(TR_CSV, "residual train csv")
    _need_file(VA_CSV, "residual val csv")
    _need_file(TE_CSV, "residual test csv")

    stats = json.loads(STATS_JSON.read_text(encoding="utf-8"))
    seq_len = int(stats["seq_len"])
    d_in = len(list(stats["features"]))

    tr = pd.read_csv(TR_CSV)
    va = pd.read_csv(VA_CSV)
    te = pd.read_csv(TE_CSV)

    _need_cols(tr, ["vin", "t_idx", "t_end", "y_true", "base_pred", "residual"], f"TRAIN:{TR_CSV.name}")
    _need_cols(va, ["vin", "t_idx", "t_end", "y_true", "base_pred", "residual"], f"VAL:{VA_CSV.name}")
    _need_cols(te, ["vin", "t_idx", "t_end", "y_true", "base_pred", "residual"], f"TEST:{TE_CSV.name}")

    # type fixes
    for df in (tr, va, te):
        df["vin"] = df["vin"].astype(str)
        df["t_idx"] = df["t_idx"].astype(np.int64)
        df["t_end"] = df["t_end"].astype(np.int64)
        for c in ["y_true", "base_pred", "residual"]:
            df[c] = df[c].astype(np.float32)

    # residual standardization based on TRAIN (OOF residuals)
    resid_mu = float(tr["residual"].mean())
    resid_std = float(tr["residual"].std(ddof=0))
    # [新增] base_pred standardization stats (只用 Train 统计)
    base_mu = float(tr["base_pred"].mean())
    base_std = float(tr["base_pred"].std(ddof=0))
    print(f"Base Pred Stats: mu={base_mu:.4f} std={base_std:.4f}")

    # 把这两个新参数传给 Dataset
    ds_tr = ResidualSeqDataset(tr, seq_len, base_mu, base_std)
    ds_va = ResidualSeqDataset(va, seq_len, base_mu, base_std)
    ds_te = ResidualSeqDataset(te, seq_len, base_mu, base_std)
    if not np.isfinite(resid_std) or resid_std <= 1e-12:
        raise RuntimeError(f"Bad residual std: {resid_std}. Check TRAIN residual file: {TR_CSV}")

    print(
        f"Start RESIDUAL+CTX | BASE={BASE_COL} MODEL={MODEL} | "
        f"seq_len={seq_len} d_in={d_in} | "
        f"train={len(tr)} val={len(va)} test={len(te)} | "
        f"LR={LR} WD={WEIGHT_DECAY} EPOCHS={EPOCHS} BATCH={BATCH_SIZE}"
    )
    print(f"Residual standardize: mu={resid_mu:.6f} std={resid_std:.6f}")

  # 正确代码（修改后）
    ds_tr = ResidualSeqDataset(tr, seq_len, base_mu, base_std)
    ds_va = ResidualSeqDataset(va, seq_len, base_mu, base_std)
    ds_te = ResidualSeqDataset(te, seq_len, base_mu, base_std)

    dl_tr = DataLoader(ds_tr, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, drop_last=False)
    dl_va = DataLoader(ds_va, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, drop_last=False)
    dl_te = DataLoader(ds_te, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, drop_last=False)

    # build model
    if MODEL == "gru":
        enc = GRUEncoder(d_in=d_in, hidden=HIDDEN, layers=LAYERS, dropout=DROPOUT)
        hdim = HIDDEN
    elif MODEL == "mlp":
        enc = MLPEncoder(d_in=d_in, hidden=HIDDEN, dropout=DROPOUT)
        hdim = HIDDEN
    elif MODEL == "tcn":
        enc = TCNEncoder(d_in=d_in, hidden=HIDDEN, layers=max(1, LAYERS), dropout=DROPOUT)
        hdim = HIDDEN
    elif MODEL == "tfmr":
        enc = TFMRDecoder(d_in=d_in, d_model=D_MODEL, nhead=NHEAD, nlayers=max(1, LAYERS), ffn=FFN, dropout=DROPOUT)
        hdim = D_MODEL
    else:
        raise ValueError(f"Unknown MODEL={MODEL}. Use gru|mlp|tcn|tfmr")

    model = ResidualCorrector(enc, h_dim=hdim, dropout=DROPOUT).to(DEVICE)

    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)
    scheduler = CosineAnnealingLR(opt, T_max=EPOCHS, eta_min=2e-5)

    # residual regression loss on standardized target
    # SmoothL1 is stable and reduces sensitivity to heavy-tail residuals
    loss_fn = nn.SmoothL1Loss(reduction="none", beta=1.0)

    best_val_final = 1e9
    best_gamma = 1.0
    best_path = RES_DIR / f"model_{OUT_TAG}.pt"

    # train loop
    for ep in range(1, EPOCHS + 1):
        model.train()
        total_loss = 0.0
        n_steps = 0

        for x, base, y_true, resid_true, _, _, _ in dl_tr:
            x = x.to(DEVICE)
            base = base.to(DEVICE)
            y_true = y_true.to(DEVICE)
            resid_true = resid_true.to(DEVICE)

            # standardize target residual
            target_std = (resid_true - resid_mu) / resid_std

            pred_std = model(x, base)  # (B,)

            per = loss_fn(pred_std, target_std)  # (B,)

            # optional: emphasize low-SoH examples
            if LOW_W > 1.0:
                w = torch.ones_like(y_true)
                w = torch.where(y_true < LOW_THR, torch.full_like(y_true, float(LOW_W)), w)
                loss = (w * per).mean()
            else:
                loss = per.mean()

            opt.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            opt.step()

            total_loss += float(loss.item())
            n_steps += 1

        scheduler.step()
        lr_now = opt.param_groups[0]["lr"]

        # evaluate on VAL
        df_va = predict_all(model, dl_va, DEVICE, resid_mu, resid_std, base_mu, base_std)
        gamma, val_final = pick_best_gamma(df_va, GAMMA_MAX, GAMMA_STEPS)
        m = compute_metrics(df_va, gamma)

        print(
            f"Epoch {ep:02d} | lr={lr_now:.2e} | train_loss={total_loss/max(1,n_steps):.6f} | "
            f"VAL base_MAE={m['base_MAE']:.6f} final_MAE={m['final_MAE']:.6f} resid_MAE={m['resid_MAE']:.6f} | gamma={gamma:.3f}"
        )

        if m["final_MAE"] < best_val_final:
            best_val_final = m["final_MAE"]
            best_gamma = gamma
            torch.save(
                {
                    "model": model.state_dict(),
                    "stats": stats,
                    "resid_mu": resid_mu,
                    "resid_std": resid_std,
                    "best_gamma": best_gamma,
                    "base_col": BASE_COL,
                    "model_type": MODEL,
                },
                best_path,
            )

    # load best
    ckpt = torch.load(best_path, map_location=DEVICE)
    model.load_state_dict(ckpt["model"])
    resid_mu = float(ckpt["resid_mu"])
    resid_std = float(ckpt["resid_std"])
    # print(f"Original Best Gamma from Val: {ckpt['best_gamma']}")
    # best_gamma = 0.15  # <--- 把它强行按在 0.1 ~ 0.2 之间
    # print(f"Forced Safe Gamma for Test: {best_gamma}")
    best_gamma = float(ckpt["best_gamma"])

    # final eval
    df_va = predict_all(model, dl_va, DEVICE, resid_mu, resid_std, base_mu, base_std)
    df_te = predict_all(model, dl_te, DEVICE, resid_mu, resid_std, base_mu, base_std)

    # use stored best_gamma; also report optimal gamma on TEST (for analysis only)
    val_metrics = compute_metrics(df_va, best_gamma)
    test_metrics = compute_metrics(df_te, best_gamma)

    print(
        f"[VAL]  base_MAE={val_metrics['base_MAE']:.6f} | final_MAE={val_metrics['final_MAE']:.6f} | "
        f"resid_MAE={val_metrics['resid_MAE']:.6f} | n={len(df_va)} | gamma={best_gamma:.3f}"
    )
    print(
        f"[TEST] base_MAE={test_metrics['base_MAE']:.6f} | final_MAE={test_metrics['final_MAE']:.6f} | "
        f"resid_MAE={test_metrics['resid_MAE']:.6f} | n={len(df_te)} | gamma={best_gamma:.3f}"
    )

    # write predictions
    def _add_final(df: pd.DataFrame, gamma: float) -> pd.DataFrame:
        df = df.copy()
        df["final_pred"] = df["base_pred"] + gamma * df["residual_pred"]
        df["abs_err_base"] = (df["y_true"] - df["base_pred"]).abs()
        df["abs_err_final"] = (df["y_true"] - df["final_pred"]).abs()
        return df

    df_va_out = _add_final(df_va, best_gamma)
    df_te_out = _add_final(df_te, best_gamma)

    out_val = RES_DIR / f"predictions_val_{OUT_TAG}.csv"
    out_test = RES_DIR / f"predictions_test_{OUT_TAG}.csv"
    df_va_out.to_csv(out_val, index=False, encoding="utf-8-sig")
    df_te_out.to_csv(out_test, index=False, encoding="utf-8-sig")

    print("Saved model:", best_path)
    print("Saved:", out_val)
    print("Saved:", out_test)


if __name__ == "__main__":
    main()
